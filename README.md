# Machine Learning - Activities
This repository contains the activities I personally done during the course **Machine Learning** as part of the Master’s Degree in **Computer Science** at UniPD, taught by Professor Fabio Aiolli for the academic year 2024/2025. 

Some exercises refer to the following books:
-  Tom Mitchell, *Machine Learning*, McGraw Hill (1997)
- Ethem Alpaydin, *Introduction to Machine Learning*, The MIT Press (2004)

## Activity 1 - DTs
Given a very simple dataset provided by the professor I built a decision tree using the ID3 algorithm.
[Activity Report](A1-DTree/PlayTennis.pdf)

## Activity 2 - Ridge Regression
Implementation of Ridge Regression using `sklearn`, where we observe how different alpha values influence the plotted models.

## Activity 3 - Exercise on DTs
This is exercise 3.2 from Mitchell's book (1997), where I calculated the entropy and information gain for a specific label. 
[Activity Report](A3-DTree/A3.pdf)

## Activity 4 - DTs overfitting
The notebook analyzes Decision Trees and overfitting using the Breast Cancer dataset from `sklearn`, exploring parameters like `max_depth` and `min_samples_split`. 

## Activity 5 - Exercise on DTs
*True or false*: If decision tree $D 2$ is an elaboration of tree $D 1$, then $D 1$ is _more-general-than_ $D 2$. Assume $D 1$ and $D 2$ are decision trees representing arbitrary boolean functions, and that $D 2$ is an elaboration of $D 1$ if ID3 could extend $D 1$ into $D 2$. If true, give a proof; if false, a counterexample. Exercise 3.3 from Mitchell's book (1997).
[Activity Report](A5-E3.3/A5.pdf)

## Activity 6 - Exercise on DTs
Give decision trees to represent some boolean functions. Exercise 3.1 from Mitchell's book (1997).
[Activity Report](A6-E3.1/A6.pdf)

## Activity 7 - Perceptron
In this exercise we need to check if Perceptron A *is more-general-than* perceptron B. Exercise 4.3 from Mitchell's book (1997).
[Activity Report](A7-E4.3/A7.pdf)

## Activity 8 - Perceptron
Perceptron implementation of the NOT operator.
[Activity Report](A8-not-perceptron/A8.pdf)

## Activity 9 - Bayesian Learning
Let us denote by x the number of spam emails I receive in a random sample of n.
Assume that the prior for q, the proportion of spam emails, is uniform in [0,1].
Find the posterior distribution for p(q|x).
[Activity Report](A9-E14.6.2/A9.pdf)

## Activity 10 - Ensemble Learning
In this activity, we will briefly compare Random Forests and Decision Trees.

## Activity 11 - Clustering
What are the similarities and diﬀerences between *average-link* clustering and *k-means*? Exercise 7.10.7 from Alpaydin's book.
[Activity Report](A11-E7.10.7/A11.pdf)

## Activity 12 -  Perceptron
Show the perceptron that calculates NAND of its two inputs. Exercise 11.14.2 from Alpaydin's book.
[Activity Report](A12-E11.14.2/A12.pdf)

## Activity 13 - Perceptron
Show the perceptron that calculates the parity of its three inputs. Exercise 11.14.3 from Alpaydin's book.
[Activity Report](A13-E11.14.3/A13.pdf)

## Activity 14 - Natural Language Processing
In this activity I compared three common models used in Natural Language Processing : Bag of Words, Word2Vec, and N-Gram.
[Activity Report](A14-NLP/A14.pdf)